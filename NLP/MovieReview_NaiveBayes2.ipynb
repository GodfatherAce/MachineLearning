{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes to classify movie reviews based on sentiment\n",
    "\n",
    "## From scratch and with Scikit-Learn\n",
    "\n",
    "We want to predict whether a review is negative or positive, based on the text of the review. We'll use Naive Bayes for our classification algorithm. A Naive Bayes classifier works by figuring out how likely data attributes are to be associated with a certain class. Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Naive Bayes?\n",
    "Naive Bayes classifier is based on Bayes' theorem, which is:\n",
    "\n",
    "$$P(H \\mid x) = \\frac{P(H) P(x \\mid H) }{P(x)}$$\n",
    "\n",
    "- P(H|x) is the posterior probability of hypothesis (H or target) given predictor (x or attribute).\n",
    "- P(H) is the prior probability of hypothesis\n",
    "- P(x|H) is the likelihood which is the probability of predictor given hypothesis.\n",
    "- P(x) is the prior probability of predictor.\n",
    "\n",
    "Naive Bayes extends Bayes' theorem to handle multiple evidences by assuming that each evidence is independent.\n",
    "\n",
    "$$ P(H \\mid x_1, \\dots, x_n) = \\frac{P(H) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "\n",
    "In most of these problems we will compare the probabilities H being true or false and the denominator will not affect the outcome so we can simply calculate the numerator.  \n",
    "\n",
    "**Example**\n",
    "![](movie_review.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll be working with a CSV file containing movie reviews. Each row contains the text of the review, as well as a number indicating whether the tone of the review is positive(1) or negative(-1).\n",
    "\n",
    "We want to predict whether a review is negative or positive, based on the text alone. To do this, we'll train an algorithm using the reviews and classifications in train.csv, and then make predictions on the reviews in test.csv. We'll be able to calculate our error using the actual classifications in test.csv to see how good our predictions were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"train.csv\", 'r') as file:\n",
    "    reviews = list(csv.reader(file))\n",
    "    \n",
    "print(len(reviews))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "\n",
    "### Obtaining P(H) : the prior probability of hypothesis (positive review)\n",
    "\n",
    "Now that we have the word counts, we just need to convert them to probabilities and multiply them out to predict the classifications.\n",
    "\n",
    "Let's start by obtaining the prior probabilities as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(H) or the prior is: 0.5007052186177715\n"
     ]
    }
   ],
   "source": [
    "# Computing the prior(H=positive reviews) according to the Naive Bayes' equation\n",
    "def get_H_count(score):\n",
    "    # Compute the count of each classification occurring in the data\n",
    "    return len([r for r in reviews if r[1] == str(score)])\n",
    "\n",
    "# We'll use these counts for smoothing when computing the prediction\n",
    "positive_review_count = get_H_count(1)\n",
    "negative_review_count = get_H_count(-1)\n",
    "\n",
    "# These are the prior probabilities (we saw them in the formula as P(H))\n",
    "prob_positive = positive_review_count / len(reviews)\n",
    "prob_negative = negative_review_count / len(reviews)\n",
    "print(\"P(H) or the prior is:\", prob_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining P(xi|H) : the likelihood\n",
    "\n",
    "#### Finding Word Counts\n",
    "\n",
    "We're trying to determine if we should classify a data row as negative or positive. The easiest way to generate features from text is to split the text up into words. Each word in a movie review will then be a feature that we can work with. To do this, we'll split the reviews based on whitespace.\n",
    "\n",
    "Afterwards, we'll count up how many times each word occurs in the negative reviews, and how many times each word occurs in the positive reviews. Eventually, we'll use the counts to compute the probability that a new review will belong to one class versus the other.\n",
    "\n",
    "[We will use the following python library](https://docs.python.org/2/library/collections.html)\n",
    "***class collections.Counter([iterable-or-mapping])***\n",
    "\n",
    "Where, a Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python class that lets us count how many times items occur in a list\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone\n",
    "    # Lowercase the text so that the algorithm doesn't see \"Not\" and \"not\" as different words, for example\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n",
    "\n",
    "def count_text(text):\n",
    "    # Split text into words based on whitespace -- simple but effective\n",
    "    words = re.split(\"\\s+\", text)\n",
    "    # Count up the occurrence of each word\n",
    "    return Counter(words)\n",
    "\n",
    "negative_text = get_text(reviews, -1)\n",
    "positive_text = get_text(reviews, 1)\n",
    "\n",
    "# Generate word counts(WC) dictionary for negative tone\n",
    "negative_WC_dict = count_text(negative_text)\n",
    "\n",
    "# Generate word counts(WC) dictionary for positive tone\n",
    "positive_WC_dict = count_text(positive_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of word 'awesome' in positive reviews 2\n",
      "count of word 'movie' in positive reviews 304\n",
      "count of word 'awesome' in negative reviews 1\n",
      "count of word 'movie' in negative reviews 376\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(\"count of word 'awesome' in positive reviews\", positive_WC_dict.get(\"awesome\"))\n",
    "print(\"count of word 'movie' in positive reviews\", positive_WC_dict.get(\"movie\"))\n",
    "\n",
    "print(\"count of word 'awesome' in negative reviews\", negative_WC_dict.get(\"awesome\"))\n",
    "print(\"count of word 'movie' in negative reviews\", negative_WC_dict.get(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining P(xi|H) the likelyhood or the probability of predictor (a word) given hypothesis (positive review)\n",
    "\n",
    "- For every word in the text, we get the number of times that word occurred in the text, text_WC_dict.get(word) \n",
    "- Multiply it with the probability of that word in Hypothesis (e.g. positive review) = (H_WC_dict.get(word) / (sum(H_WC_dict.values())\n",
    "\n",
    "\n",
    "```python\n",
    "prediction =  text_WC_dict.get(word) * (H_WC_dict.get(word) / (sum(H_WC_dict.values())\n",
    "```\n",
    "\n",
    "We add 1 to smooth the value, smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data and correspondingly smooth the denominator counts to keep things even.\n",
    "\n",
    "After smoothing above equation becomes:\n",
    "\n",
    "```python\n",
    "prediction =  text_WC_dict.get(word) * (H_WC_dict.get(word)+1) / (sum(H_WC_dict.values()+ H_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# H = positive review or negative review\n",
    "def make_class_prediction(text, H_WC_dict, H_prob, H_count):\n",
    "    prediction = 1\n",
    "    text_WC_dict = count_text(text)\n",
    "    \n",
    "    for word in text_WC_dict:       \n",
    "        prediction *=  text_WC_dict.get(word,0) * ((H_WC_dict.get(word, 0) + 1) / (sum(H_WC_dict.values()) + H_count))\n",
    "\n",
    "        # Now we multiply by the probability of the class existing in the documents\n",
    "    return prediction * H_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this review: plot : two teen couples go to a church party drink and then drive . they get into an accident . one of the guys dies but his girlfriend continues to see him in her life and has nightmares . what's the deal ? watch the movie and \" sorta \" find out . . . critique : a mind-fuck movie for the teen generation that touches on a very cool idea but presents it in a very bad package . which is what makes this review an even harder one to write since i generally applaud films which attempt\n",
      "\n",
      "The predicted label is  -1\n",
      "The actual label is  -1\n"
     ]
    }
   ],
   "source": [
    "# Now we can generate probabilities for the classes our reviews belong to\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater\n",
    "def make_decision(text):\n",
    "    \n",
    "    # Compute the negative and positive probabilities\n",
    "    negative_prediction = make_class_prediction(text, negative_WC_dict, prob_negative, negative_review_count)\n",
    "    positive_prediction = make_class_prediction(text, positive_WC_dict, prob_positive, positive_review_count)\n",
    "\n",
    "    # We assign a classification based on which probability is greater\n",
    "    if negative_prediction > positive_prediction:\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "print(\"For this review: {0}\".format(reviews[0][0]))\n",
    "print(\"\")\n",
    "print(\"The predicted label is \", make_decision(reviews[0][0]))\n",
    "print(\"The actual label is \", reviews[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_decision(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_decision(\"best movie ever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we can make predictions, let's predict the probabilities for the reviews in test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test.csv\", 'r') as file:\n",
    "    test = list(csv.reader(file))\n",
    "\n",
    "predictions = [make_decision(r[0]) for r in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the predictions: 0.680701754385965\n"
     ]
    }
   ],
   "source": [
    "actual = [int(r[1]) for r in test]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate the ROC curve using scikits-learn\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "\n",
    "# Measure the area under the curve\n",
    "# The closer to 1 it is, the \"better\" the predictions\n",
    "print(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes implementation in scikit-learn\n",
    "\n",
    "There are a lot of extensions we could add to this algorithm to make it perform better. We could remove punctuation and other non-characters. We could remove stopwords, or perform stemming or lemmatization.\n",
    "\n",
    "We don't want to have to code the entire algorithm out every time, though. An easier way to use Naive Bayes is to use the implementation in scikit-learn. Scikit-learn is a Python machine learning library that contains implementations of all the common machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomal naive bayes AUC: 0.635500515995872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate counts from text using a vectorizer  \n",
    "# We can choose from other available vectorizers, and set many different options\n",
    "# This code performs our step of computing word counts\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=.05)\n",
    "train_features = vectorizer.fit_transform([r[0] for r in reviews])\n",
    "test_features = vectorizer.transform([r[0] for r in test])\n",
    "\n",
    "# Fit a Naive Bayes model to the training data\n",
    "# This will train the model using the word counts we computed and the existing classifications in the training set\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_features, [int(r[1]) for r in reviews])\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features\n",
    "predictions = nb.predict(test_features)\n",
    "\n",
    "# Compute the error\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "print(\"Multinomal naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
