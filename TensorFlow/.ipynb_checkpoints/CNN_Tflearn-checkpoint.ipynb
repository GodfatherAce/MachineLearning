{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TFLearn -  Convolutional Neural Network Model\n",
    "\n",
    "In this tutorial we'll continue working with MNIST dataset. MNIST is a simple computer vision dataset and consists of images of handwritten digits like the image below. In [my first project](http://suruchifialoke.com/2017-06-15-predicting-digits_tensorflow/) on this series, I trained a basic neural network to distinguish digits from 0 to 9 on this dataset. Then, in [the second tutorial](http://suruchifialoke.com/2017-06-17-predicting-digits-cnn-tensorflow/), I implemented a Convolutional Neural Network (ConvNet) using TensorFlow. In this tutorial I inplement the ConvNet using TFLearn, which is a high-level/abstraction layer for TensorFlow. The reason for using TFLearn or similar abstractions such as Keras, SKFlow, and TFSlim is to bypass the verbosity of the TensorFlow code. So let's begin.\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [About TFLearn](http://tflearn.org/)\n",
    "- [About MNIST](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "- [My basic Neural Network project on this dataset](http://suruchifialoke.com/2017-06-15-predicting-digits_tensorflow/)\n",
    "- [My TensorFlow implementation of ConvNet on this dataset](http://suruchifialoke.com/2017-06-17-predicting-digits-cnn-tensorflow/)\n",
    "- [Tutorial by PythonProgramming.net](https://pythonprogramming.net/convolutional-neural-network-cnn-machine-learning-tutorial/?completed=/rnn-tensorflow-python-machine-learning-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the MNIST Data\n",
    "\n",
    "We're going to be working first with the MNIST dataset, which is a dataset that contains 60,000 training samples and 10,000 testing samples of hand-written and labeled digits, 0 through 9, so ten total \"classes.\" \n",
    "\n",
    "The MNIST dataset has the images as purely black and white, of size 28 x 28, or 784 pixels total. \n",
    "\n",
    "Our features will be the pixel values for each pixel, thresholded. Either the pixel is \"blank\" (nothing there, a 0), or there is something there (1). Those are our features. We're going to attempt to just use this extremely rudimentary data, and predict the number we're looking at (a 0,1,2,3,4,5,6,7,8, or 9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "X, Y, test_x, test_y = mnist.load_data(one_hot=True)\n",
    "\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "test_x = test_x.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Computation Model\n",
    "\n",
    "We have imported a function for the convolution and pooling.  We have also imported fully_connected and regression. Then, we load in the data, and reshape the data. Now we are going to begin building the convolutional neural network, starting with the input layer:\n",
    "\n",
    "** Definitions of the layers **\n",
    "\n",
    "- Each input image had a 28x28 pixels - > Reshape input to a 4D tensor \n",
    "- Take 5x5 convolutions on the initial image, and get 32 outputs and apply ReLU\n",
    "- Next, take 5x5 convolutions of the 32 inputs and make 64 output and apply RelU. \n",
    "- We're left with 64 of 7x7 sized images (It went through two max pooling process. And each time, it reduces the dimension by half because of its stride of 2 and size of 2. Hence, 28/2/2 = 7)\n",
    "- then we're outputting to 1024 nodes in the fully connected layer and applying ReLu\n",
    "- Then, the output layer is 1024 layers, to 10, which are the final 10 possible outputs for the actual label itself (0-9).\n",
    "\n",
    "* The ReLU rectifier is an activation function defined as f(x)=max(0,x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input layer\n",
    "convnet = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "\n",
    "# 2 layers of convolution and pooling:\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "# Fully connected layer\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "# Output Layer\n",
    "convnet = fully_connected(convnet, 10, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=0.01,\n",
    "                     loss='categorical_crossentropy', name='targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3245  | total loss: \u001b[1m\u001b[32m0.05677\u001b[0m\u001b[0m | time: 83.855s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 004 | loss: 0.05677 - acc: 0.9827 -- iter: 42560/55000\n"
     ]
    }
   ],
   "source": [
    "# Create the model \n",
    "\n",
    "model = tflearn.DNN(convnet)\n",
    "\n",
    "model.fit({'input': X}, {'targets': Y}, n_epoch=10, \n",
    "          validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "          snapshot_step=500, show_metric=True, run_id='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our [basic neural network model](http://suruchifialoke.com/2017-06-15-predicting-digits_tensorflow/) gave an accuracy of ~95% for 10 epochs. This ConvNet model certainly gives better accuracy of ~98% for the same number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
